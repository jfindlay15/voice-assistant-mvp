# Point the container to the host's Ollama server
LLM__BASE_URL=http://host.docker.internal:11434

# Ollama doesn’t need an API key — leave blank
LLM__API_KEY=

# Use the exact model you’ve pulled in Ollama (examples)
LLM__MODEL=llama3.1:8b
# e.g. llama3.1:8b  | llama3.2:3b  | qwen2.5:7b  | phi3:latest
LLM_TEMP=0.7
